{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attention.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Importing Dependencies"
      ],
      "metadata": {
        "id": "P8-GV3CFfSLK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "y1c7I60Be4B8"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import array\n",
        "from pickle import load\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import sentence_bleu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, time, os, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import re"
      ],
      "metadata": {
        "id": "jov4sd6xfZZY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle"
      ],
      "metadata": {
        "id": "orD1NYtzgPf7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense, BatchNormalization\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.merge import add\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle"
      ],
      "metadata": {
        "id": "pNm7ZO0shOE4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import pickle\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "MxZ9nQh3hqZK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "K-Ck19wYqb7X"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from IPython import display\n",
        "import time"
      ],
      "metadata": {
        "id": "rzJNLrtoz8tx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "qymjXYfMmhjJ",
        "outputId": "63322194-e475-4157-fd71-eb512288fc17"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms)\u001b[0m\n\u001b[1;32m    107\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m       ephemeral=True)\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral)\u001b[0m\n\u001b[1;32m    126\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     _message.blocking_request(\n\u001b[0;32m--> 128\u001b[0;31m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m   \u001b[0mmountpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    173\u001b[0m   request_id = send_request(\n\u001b[1;32m    174\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 175\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd '/content/drive/MyDrive/Big Data'"
      ],
      "metadata": {
        "id": "LI1O_9MimqV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading the Images"
      ],
      "metadata": {
        "id": "QNReoK4fkk6T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Flikr8k Dataset"
      ],
      "metadata": {
        "id": "EPyi1xFBi3jU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading the captions and images"
      ],
      "metadata": {
        "id": "s_eJyRDdlgH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "captions_path = '/content/drive/MyDrive/Big Data/captions/captions.txt'\n",
        "images_path = '/content/drive/MyDrive/Big Data/images'"
      ],
      "metadata": {
        "id": "LDcvYv6nipt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating a Dataframe"
      ],
      "metadata": {
        "id": "IBiZbe51niyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(captions_path, 'r')\n",
        "captions = f.read().split(\"\\n\")\n",
        "f.close()"
      ],
      "metadata": {
        "id": "MRgjIaC4nhri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "captions[:5]"
      ],
      "metadata": {
        "id": "Hlvs1dmdoE80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = []\n",
        "descs = []\n",
        "for i in captions[1:-1]:\n",
        "  # print(i)\n",
        "  id = i.split(',')[0]\n",
        "  desc = i.split(',')[1]\n",
        "  ids.append(id)\n",
        "  descs.append(desc)"
      ],
      "metadata": {
        "id": "ZAd9mS3JoLSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame()\n",
        "df['id'] = ids\n",
        "df['captions'] = descs"
      ],
      "metadata": {
        "id": "mbWtQH5wqIxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "0XymWfTJqYAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "kFwWG9h7qxT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Displaying images from the dataset"
      ],
      "metadata": {
        "id": "gOvmi3TC42zN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random \n",
        "\n",
        "sNum = random.randint(0,40455)\n",
        "k = df['id'][sNum]\n",
        "PATH = images_path + '/' + k\n",
        "\n",
        "\n",
        "ex = mpimg.imread(PATH)\n",
        "plt.imshow(ex);\n",
        "plt.axis('off')\n",
        "display.display(plt.gcf())\n",
        "display.clear_output(wait=True)\n",
        "plt.show()\n",
        "ex_cap = df['captions'].loc[df['id'] == k].values\n",
        "ex_cap"
      ],
      "metadata": {
        "id": "y3mvNo4crTq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a List of Filenames\n",
        "\n",
        "df['all_ids'] = images_path + '/' + df['id'].astype(str)\n",
        "images_list = df[\"all_ids\"].tolist()\n",
        "df = df.drop('all_ids', axis = 1)\n",
        "images_list[:5]\n"
      ],
      "metadata": {
        "id": "QB0b3EGC6BKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text Preprocessing\n",
        "\n",
        "- Remove Punctuations\n",
        "- Remove single letters\n",
        "- Remove any numbers\n",
        "- add tags to the text"
      ],
      "metadata": {
        "id": "CE3u3fohwTUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def remove_punctuations(text_original):\n",
        "    for punctuation in string.punctuation:\n",
        "        text_new = text_original.replace(punctuation, '')\n",
        "    return text_new"
      ],
      "metadata": {
        "id": "VhanBE5v4cIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_single_chars(text_original):\n",
        "  text_new = \"\"\n",
        "  for w in text_original.split():\n",
        "    if len(w) > 1:\n",
        "      text_new = text_new + \" \" + w\n",
        "  return text_new\n"
      ],
      "metadata": {
        "id": "cfH1C2x8xZeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_numbers(text_original) :\n",
        "   text_new = \"\"\n",
        "   for w in text_original.split():\n",
        "\n",
        "       isalpha = w.isalpha()\n",
        "       if isalpha:\n",
        "           text_new += \" \" + w\n",
        "   return(text_new)"
      ],
      "metadata": {
        "id": "Ye53hWHVyoFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleaned_text(text_original):\n",
        "  text = remove_punctuations(text_original)\n",
        "  text1 = remove_single_chars(text)\n",
        "  clean_text = remove_numbers(text1)\n",
        "  return(clean_text)\n"
      ],
      "metadata": {
        "id": "c90OMV14z6et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, caption in enumerate(df.captions.values):\n",
        "   newcaption = cleaned_text(caption)\n",
        "   df[\"captions\"].iloc[i] = newcaption"
      ],
      "metadata": {
        "id": "pwWAnRW80QD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "DiJ_faqp2YFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['all_captions'] = '<start>' + df['captions'].astype(str) + \"<end>\"\n",
        "captions_list = df[\"all_captions\"].tolist()\n",
        "df = df.drop('all_captions', axis = 1)\n",
        "captions_list[:5]\n"
      ],
      "metadata": {
        "id": "ribyickf3LYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Determining the Batch Size"
      ],
      "metadata": {
        "id": "R7t6IekF9nPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"length of captions :  {len(captions_list)}\")\n",
        "print(f\"length of images :  {len(images_list)}\")"
      ],
      "metadata": {
        "id": "RxRgJi7550Wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To allow better processing speed, a batch size of 64 is choosen. To round this up, 40000 samples are chosen"
      ],
      "metadata": {
        "id": "xf9CHtaH-Fwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_captions, images_list  = shuffle(captions_list,images_list,random_state=1)\n",
        "train_captions = captions_list[:40000]\n",
        "images_list = images_list[:40000]\n",
        "\n",
        "print(f\"New length of captions :  {len(train_captions)}\")\n",
        "print(f\"New length of images :  {len(images_list)}\")"
      ],
      "metadata": {
        "id": "J4O03FQk97Ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xeC7RPzTAPdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Def"
      ],
      "metadata": {
        "id": "gf3TF_onNzEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(image_path):\n",
        "   img = tf.io.read_file(image_path)\n",
        "   img = tf.image.decode_jpeg(img, channels=3)\n",
        "   img = tf.image.resize(img, (224, 224))\n",
        "   img = preprocess_input(img)\n",
        "   return img, image_path\n",
        "\n",
        "image_model = tf.keras.applications.VGG16(include_top=False, weights='imagenet')\n",
        "new_input = image_model.input\n",
        "hidden_layer = image_model.layers[-1].output\n",
        "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n",
        "\n",
        "image_features_extract_model.summary()"
      ],
      "metadata": {
        "id": "-z2K_4JKS8iT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encode_train = sorted(set(img_name_vector))\n",
        "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
        "image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(64)"
      ],
      "metadata": {
        "id": "PX96edL4S8dL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "for img, path in tqdm(image_dataset):\n",
        " batch_features = image_features_extract_model(img)\n",
        " batch_features = tf.reshape(batch_features,\n",
        "                             (batch_features.shape[0], -1, batch_features.shape[3]))\n",
        "\n",
        " for bf, p in zip(batch_features, path):\n",
        "   path_of_feature = p.numpy().decode(\"utf-8\")\n",
        "   np.save(path_of_feature, bf.numpy())"
      ],
      "metadata": {
        "id": "P47stqVuS8Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_k = 5000\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
        "                                                 oov_token=\"<unk>\",\n",
        "                                                 filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
        "\n",
        "tokenizer.fit_on_texts(train_captions)\n",
        "train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
        "tokenizer.word_index['<pad>'] = 0\n",
        "tokenizer.index_word[0] = '<pad>'\n",
        "\n",
        "train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
        "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')"
      ],
      "metadata": {
        "id": "XY9973_tS8Xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_captions[:3]"
      ],
      "metadata": {
        "id": "dsoP9rloS8Uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_seqs[:3]"
      ],
      "metadata": {
        "id": "1nEgVP5mS8Sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_max_length(tensor):\n",
        "   return max(len(t) for t in tensor)\n",
        "max_length = calc_max_length(train_seqs)\n",
        "\n",
        "def calc_min_length(tensor):\n",
        "   return min(len(t) for t in tensor)\n",
        "min_length = calc_min_length(train_seqs)\n",
        "\n",
        "print('Max Length of any caption : Min Length of any caption = '+ str(max_length) +\" : \"+str(min_length))"
      ],
      "metadata": {
        "id": "bpyuqrS6S7-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector,cap_vector, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "pVhbysGSS77y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "embedding_dim = 256\n",
        "units = 512\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "num_steps = len(img_name_train) // BATCH_SIZE\n",
        "features_shape = 512\n",
        "attention_features_shape = 49"
      ],
      "metadata": {
        "id": "L-jQzwKZS71i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_func(img_name, cap):\n",
        " img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
        " return img_tensor, cap\n",
        "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
        "\n",
        "# Use map to load the numpy files in parallel\n",
        "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
        "        map_func, [item1, item2], [tf.float32, tf.int32]),\n",
        "         num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "zsOxlqpqS7yV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Attention Model"
      ],
      "metadata": {
        "id": "plhyQz6CTmF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG16_Encoder(tf.keras.Model):\n",
        "   # This encoder passes the features through a Fully connected layer\n",
        "   def __init__(self, embedding_dim):\n",
        "       super(VGG16_Encoder, self).__init__()\n",
        "       # shape after fc == (batch_size, 49, embedding_dim)\n",
        "       self.fc = tf.keras.layers.Dense(embedding_dim)\n",
        "       self.dropout = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)\n",
        "\n",
        "   def call(self, x):\n",
        "       #x= self.dropout(x)\n",
        "       x = self.fc(x)\n",
        "       x = tf.nn.relu(x)\n",
        "       return x"
      ],
      "metadata": {
        "id": "_y0FUMLhS7uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rnn_type(units):\n",
        "   if tf.test.is_gpu_available():\n",
        "       return tf.compat.v1.keras.layers.CuDNNLSTM(units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "   else:\n",
        "       return tf.keras.layers.GRU(units,\n",
        "                                  return_sequences=True,\n",
        "                                  return_state=True,\n",
        "                                  recurrent_activation='sigmoid',\n",
        "                                  recurrent_initializer='glorot_uniform')"
      ],
      "metadata": {
        "id": "uyWbLZ6eS7rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Rnn_Local_Decoder(tf.keras.Model):\n",
        " def __init__(self, embedding_dim, units, vocab_size):\n",
        "   super(Rnn_Local_Decoder, self).__init__()\n",
        "   self.units = units\n",
        "   self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "   self.gru = tf.keras.layers.GRU(self.units,\n",
        "                                  return_sequences=True,\n",
        "                                  return_state=True,\n",
        "                                  recurrent_initializer='glorot_uniform')\n",
        "  \n",
        "   self.fc1 = tf.keras.layers.Dense(self.units)\n",
        "\n",
        "   self.dropout = tf.keras.layers.Dropout(0.5, noise_shape=None, seed=None)\n",
        "   self.batchnormalization = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n",
        "\n",
        "   self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "   # Implementing Attention Mechanism\n",
        "   self.Uattn = tf.keras.layers.Dense(units)\n",
        "   self.Wattn = tf.keras.layers.Dense(units)\n",
        "   self.Vattn = tf.keras.layers.Dense(1)\n",
        "\n",
        " def call(self, x, features, hidden):\n",
        "   # features shape ==> (64,49,256) ==> Output from ENCODER\n",
        "   # hidden shape == (batch_size, hidden_size) ==>(64,512)\n",
        "   # hidden_with_time_axis shape == (batch_size, 1, hidden_size) ==> (64,1,512)\n",
        "\n",
        "   hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "\n",
        "   # score shape == (64, 49, 1)\n",
        "   # Attention Function\n",
        "   '''e(ij) = f(s(t-1),h(j))'''\n",
        "   ''' e(ij) = Vattn(T)*tanh(Uattn * h(j) + Wattn * s(t))'''\n",
        "\n",
        "   score = self.Vattn(tf.nn.tanh(self.Uattn(features) + self.Wattn(hidden_with_time_axis)))\n",
        "\n",
        "   # self.Uattn(features) : (64,49,512)\n",
        "   # self.Wattn(hidden_with_time_axis) : (64,1,512)\n",
        "   # tf.nn.tanh(self.Uattn(features) + self.Wattn(hidden_with_time_axis)) : (64,49,512)\n",
        "   # self.Vattn(tf.nn.tanh(self.Uattn(features) + self.Wattn(hidden_with_time_axis))) : (64,49,1) ==> score\n",
        "\n",
        "   # you get 1 at the last axis because you are applying score to self.Vattn\n",
        "   # Then find Probability using Softmax\n",
        "   '''attention_weights(alpha(ij)) = softmax(e(ij))'''\n",
        "\n",
        "   attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "   # attention_weights shape == (64, 49, 1)\n",
        "   # Give weights to the different pixels in the image\n",
        "   ''' C(t) = Summation(j=1 to T) (attention_weights * VGG-16 features) '''\n",
        "\n",
        "   context_vector = attention_weights * features\n",
        "   context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "   # Context Vector(64,256) = AttentionWeights(64,49,1) * features(64,49,256)\n",
        "   # context_vector shape after sum == (64, 256)\n",
        "   # x shape after passing through embedding == (64, 1, 256)\n",
        "\n",
        "   x = self.embedding(x)\n",
        "   # x shape after concatenation == (64, 1,  512)\n",
        "\n",
        "   x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "   # passing the concatenated vector to the GRU\n",
        "\n",
        "   output, state = self.gru(x)\n",
        "   # shape == (batch_size, max_length, hidden_size)\n",
        "\n",
        "   x = self.fc1(output)\n",
        "   # x shape == (batch_size * max_length, hidden_size)\n",
        "\n",
        "   x = tf.reshape(x, (-1, x.shape[2]))\n",
        "\n",
        "   # Adding Dropout and BatchNorm Layers\n",
        "   x= self.dropout(x)\n",
        "   x= self.batchnormalization(x)\n",
        "\n",
        "   # output shape == (64 * 512)\n",
        "   x = self.fc2(x)\n",
        "\n",
        "   # shape : (64 * 8329(vocab))\n",
        "   return x, state, attention_weights\n",
        "\n",
        " def reset_state(self, batch_size):\n",
        "   return tf.zeros((batch_size, self.units))\n",
        "\n",
        "\n",
        "encoder = VGG16_Encoder(embedding_dim)\n",
        "decoder = Rnn_Local_Decoder(embedding_dim, units, vocab_size)"
      ],
      "metadata": {
        "id": "7Fq73cYbS7oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "   from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        " mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        " loss_ = loss_object(real, pred)\n",
        " mask = tf.cast(mask, dtype=loss_.dtype)\n",
        " loss_ *= mask\n",
        "\n",
        " return tf.reduce_mean(loss_)"
      ],
      "metadata": {
        "id": "5lPoMorPS7Z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_plot = []\n",
        "\n",
        "@tf.function\n",
        "def train_step(img_tensor, target):\n",
        " loss = 0\n",
        " # initializing the hidden state for each batch\n",
        " # because the captions are not related from image to image\n",
        "\n",
        " hidden = decoder.reset_state(batch_size=target.shape[0])\n",
        " dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        " with tf.GradientTape() as tape:\n",
        "     features = encoder(img_tensor)\n",
        "     for i in range(1, target.shape[1]):\n",
        "         # passing the features through the decoder\n",
        "         predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
        "         loss += loss_function(target[:, i], predictions)\n",
        "\n",
        "         # using teacher forcing\n",
        "         dec_input = tf.expand_dims(target[:, i], 1)\n",
        "\n",
        " total_loss = (loss / int(target.shape[1]))\n",
        " trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        " gradients = tape.gradient(loss, trainable_variables)\n",
        " optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        " return loss, total_loss"
      ],
      "metadata": {
        "id": "l78zHWTXT6Jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "EPOCHS = 20\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "   start = time.time()\n",
        "   total_loss = 0\n",
        "\n",
        "   for (batch, (img_tensor, target)) in enumerate(dataset):\n",
        "       batch_loss, t_loss = train_step(img_tensor, target)\n",
        "       total_loss += t_loss\n",
        "\n",
        "       if batch % 100 == 0:\n",
        "           print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
        "             epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
        "   # storing the epoch end loss value to plot later\n",
        "   loss_plot.append(total_loss / num_steps)\n",
        "\n",
        "   print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
        "                                        total_loss/num_steps))\n",
        "\n",
        "   print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "metadata": {
        "id": "q3K13gdsT9ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss_plot)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DLCFPNkGUBQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(image):\n",
        "   attention_plot = np.zeros((max_length, attention_features_shape))\n",
        "\n",
        "   hidden = decoder.reset_state(batch_size=1)\n",
        "   temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
        "   img_tensor_val = image_features_extract_model(temp_input)\n",
        "   img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
        "\n",
        "   features = encoder(img_tensor_val)\n",
        "   dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
        "   result = []\n",
        "\n",
        "   for i in range(max_length):\n",
        "       predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
        "       attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
        "       predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "       result.append(tokenizer.index_word[predicted_id])\n",
        "\n",
        "       if tokenizer.index_word[predicted_id] == '<end>':\n",
        "           return result, attention_plot\n",
        "\n",
        "       dec_input = tf.expand_dims([predicted_id], 0)\n",
        "   attention_plot = attention_plot[:len(result), :]\n",
        "\n",
        "   return result, attention_plot"
      ],
      "metadata": {
        "id": "tvSnG5aGUXrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_attention(image, result, attention_plot):\n",
        "   temp_image = np.array(Image.open(image))\n",
        "   fig = plt.figure(figsize=(10, 10))\n",
        "   len_result = len(result)\n",
        "   for l in range(len_result):\n",
        "       temp_att = np.resize(attention_plot[l], (8, 8))\n",
        "       ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n",
        "       ax.set_title(result[l])\n",
        "       img = ax.imshow(temp_image)\n",
        "       ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
        "\n",
        "   plt.tight_layout()\n",
        "   plt.show()"
      ],
      "metadata": {
        "id": "Yk-N40ifUYeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RRIv6qcXUba8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}